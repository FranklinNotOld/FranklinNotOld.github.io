<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Publication</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<style>
            /* Flex container styles */
            .container {
                display: flex;
                justify-content: space-between;
                align-items: center;
            }
            
            /* Text container styles */
            .text-container {
                flex: 1;
                text-align: justify;
            }
            
            /* Image container styles */
            .image-container {
                flex: 0 0 auto; /* Prevent image container from growing */
            }
            
            /* Image styles */
            .image-container img {
                max-width: 450px; /* Adjust width as needed */
                height: auto; /* Maintain aspect ratio */
                border-radius: 8px; /* Optional: add rounded corners */
                margin-left: 25px; /* Space between text and image */
            }
        </style>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
                            <header id="header">
                                <a href="self.html" class="logo"><strong>Yongkang Li </strong> homepage</a>
                            </header>
                        
                        	<!-- Banner -->
                            <section id="banner">
                                <div class="content">
                                    <header>
                                        <h1 style="font-size: 36px;">Real-time Accident Anticipation for Autonomous Driving Through Monocular Depth-Enhanced 3D Modeling</h1>
                                    </header>
									<div style="font-size: 16px;">Haicheng Liao*, Yongkang Li*, Zhenning Li, Zilin Bian, Jaeyoung Lee, Zhiyong Cui, Guohui Zhang and Chengzhong Xu</div>
                                </div>
                            </section>
                            <!-- <hr style="border-style: dashed; margin-top: -35px;" class="major" /> -->

							<!-- Section -->
							<section >
								<header class="main">
									<h1 id="abstract" style="font-size: 28px; margin-top: -20px; margin-bottom: 5px;">Abstract</h1>
								</header>

								<span class="image fit">
									<img src="../../images/paper/aap-head.png" alt="" style="width: 60%; display: block; margin: 0 auto;"/>
								</span>
				
								<p style="font-size: 16px;">
									The primary goal of traffic accident anticipation is to foresee potential accidents in real time using dashcam videos, a task that is pivotal for enhancing the safety and reliability of autonomous driving technologies. 
									In this study, we introduce an innovative framework, AccNet, which significantly advances the prediction capabilities beyond the current state-of-the-art 2D-based methods by incorporating monocular depth cues for sophisticated 3D scene modeling. 
									Addressing the prevalent challenge of skewed data distribution in traffic accident datasets, we propose the Binary Adaptive Loss for Early Anticipation (BA-LEA). This novel loss function, together with a multi-task learning strategy, 
									shifts the focus of the predictive model towards the critical moments preceding an accident. We rigorously evaluate the performance of our framework on three benchmark datasets—Dashcam Accident Dataset (DAD), Car Crash Dataset (CCD), AnAn Accident Detection (A3D), 
									and DADA-2000 Dataset—demonstrating its superior predictive accuracy through key metrics such as Average Precision (AP) and mean Time-To-Accident (mTTA). 
								</p>
							</section>

							<!-- Section -->
							<section>
								<div>
									<header>
										<h1 id="model-structure" style="font-size: 28px; margin-top: -20px; margin-bottom: 10px;">Model Structure</h1>
									</header>
								</div>
								<div style="width: 100%;">
									<span class="image fit">
										<img src="../../images/paper/aap-structure.png" alt="" style="width: 80%; display: block; margin: 0 auto;"/>
									</span>
								</div>																																			
								<p style="font-size: 16px;">
									Overview of the Model Structure:</p>

									<p style="font-size: 16px;">1. <b>Monocular Depth-Enhanced 3D Modeling</b>: Traditional models often rely on 2D pixel distances, which can miss crucial spatial dynamics in traffic scenes. 
										AccNet overcomes this by using monocular depth estimation to extract precise 3D coordinates of traffic participants (like vehicles and pedestrians). 
										This allows for a more accurate depiction of real-world distances and interactions between objects, which is critical for predicting accidents. The model uses the ZoeDepth algorithm to enhance depth perception from dashcam videos. 
										By calculating the 3D positions of objects in each frame, AccNet improves the ability to detect potential collisions by understanding the scene in three dimensions rather than just two.</p>

									<p style="font-size: 16px;">2. <b>Context and Object Attention Mechanisms</b>: </p>
									<ul>
										<li style="font-size: 16px;"><b>Context Attention</b>: This mechanism uses multi-head attention to focus on the most relevant parts of the visual context in each video frame, 
											ensuring that the model prioritizes critical environmental features that could indicate a potential accident.</li>
										<li style="font-size: 16px;"><b>Object Attention</b>: Similarly, this component dynamically allocates attention across different objects in the scene, 
											ensuring that the model accurately tracks and assesses the risk posed by each object, particularly those that are most likely to be involved in a collision.</li>
									</ul>

									<p style="font-size: 16px;">3. <b>3D Collision Module</b>: This module calculates the real-world distances between objects based on their 3D coordinates, which are derived from the depth-enhanced modeling. 
										It then constructs a graph representing the spatial relationships between these objects, allowing the model to better understand potential collision scenarios. By focusing on the actual 3D spatial dynamics, 
										the 3D Collision Module provides a more reliable basis for predicting accidents compared to traditional methods that rely on 2D pixel measurements. Combined with a multi-task learning strategy, 
										BA-LEA ensures that the model learns to prioritize accident prediction while maintaining stability and generalizability across various driving scenarios.</p>

									<p style="font-size: 16px;">4. <b>Monocular Depth-Enhanced 3D Modeling</b>: This novel loss function is designed to address the data imbalance issue common in accident prediction tasks, where non-accident scenarios dominate the data. 
										BA-LEA shifts the model's focus toward critical pre-accident moments, improving its ability to predict accidents early and accurately. </p>
							</section>

							<!-- Section -->
							<section>
								<div>
									<div>
										<header>
											<h1 id="experiment-result" style="font-size: 28px; margin-top: -20px; margin-bottom: 10px;">Experiment Result</h1>
										</header>
									</div>
									<div style="width: 100%;">
										<span class="image fit">
											<img src="../../images/paper/aap-visualization.png" alt="" style="width: 75%; display: block; margin: 0 auto;"/>
										</span>
									</div>
	
									<p style="font-size: 16px;">
										Our experiments on the DAD, CCD, and A3D datasets demonstrate the exceptional performance of our model on all three datasets. These evaluation results indicate an inverse relationship between AP and mTTA, suggesting that a higher AP typically results in a shorter mTTA. 
										The presented results demonstrate a balance between the two metrics, indicating that our model achieved the highest AP values across the datasets, outperforming the second-best by 2.6% on the DAD dataset. AccNet also surpasses every other model in terms of mTTA. 
										Notably, UString is significantly outperformed by our model in AP by 7.1% on DAD and 1.9% on A3D, demonstrating superior overall performance.
										To demonstrate the superiority of our model, we compared the best AP among the models. Since there were only small differences between the models on the CCD and A3D datasets, we primarily focused on the DAD dataset. AccNet achieved the highest AP value, 
										along with the corresponding highest mTTA. Moreover, our model achieved the highest AUC, indicating that it not only effectively distinguishes accident videos but also ensures the highest accuracy. 
										However, it is important to note that our TTA@80 and TTA@50 performance are slightly lower than that of DSTA, indicating that our model takes a more cautious approach in identifying the majority of positive samples. 
										Notably, as comparing the best mTTA without considering AP does not provide meaningful insights, such a comparison is not included in our analysis.
									</p>
								</div>
							</section>
						</div>
					</div>

					<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Search -->
								<section id="search" class="alt">
									<form method="post" action="#">
										<input type="text" name="query" id="query" placeholder="Search" />
									</form>
								</section>

								<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
										<!-- <li><a href="index.html">Homepage</a></li> -->
                                        <li>
											<a href="../../index.html" class="opener">Homepage</a>
											<ul>
												<li><a href="../../index.html#self-introduction">SELF-INTRODUCTION</a></li>
												<li><a href="../../index.html#news">NEWS</a></li>
												<li><a href="../../index.html#research">RESEARCH</a></li>
                                                <li><a href="../../index.html#software-copyright">SOFTWARE COPYRIGHT</a></li>
                                                <li><a href="../../index.html#project">PROJECT</a></li>
												<li><a href="../../index.html#scholarship-and-awards">SCHOLARSHIP AND AWARDS</a></li>
                                                <li><a href="../../index.html#leadership-and-activities">LEADERSHIP AND ACTIVITIES</a></li>
											</ul>
                                        </li>
                                        <li>
                                            <a href="publication/publication.html" class="opener">Publication</a>
											<ul>
												<li><a href="../publication.html#2024">2024</a></li>
                                                <ul>
                                                    <li><a href="mm24.html">When, Where, and What? A Benchmark for Accident Anticipation and Localization with Large Language Models</a></li>
                                                    <li><a href="ecai24.html">Less is More: Efficient Brain-Inspired Learning for Autonomous Driving Trajectory Prediction</a></li>
                                                    <li><a href="aap.html">Real-time Accident Anticipation for Autonomous Driving Through Monocular Depth-Enhanced 3D Modeling</a></li>
                                                    <li><a href="tiv.html">A Cognitive-Based Trajectory Prediction Approach for Autonomous Driving</a></li>
                                                </ul>
												<li><a href="../publication.html#2023">2023</a></li>
                                                <ul>
                                                    <li><a href="#">Lightweight Human Observation-inspired Trajectory Prediction For Autonomous Driving</a></li>
                                                </ul>
											</ul>
										</li>
									</ul>
								</nav>

						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/browser.min.js"></script>
			<script src="../../assets/js/breakpoints.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<script src="../../assets/js/main.js"></script>

	</body>
</html>