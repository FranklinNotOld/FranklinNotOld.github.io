<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Project1</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
		<style>
            /* Flex container styles */
            .container {
                display: flex;
                justify-content: space-between;
                align-items: center;
            }
            
            /* Text container styles */
            .text-container {
                flex: 1;
                text-align: justify;
            }
            
            /* Image container styles */
            .image-container {
                flex: 0 0 auto; /* Prevent image container from growing */
            }
            
            /* Image styles */
            .image-container img {
                max-width: 450px; /* Adjust width as needed */
                height: auto; /* Maintain aspect ratio */
                border-radius: 8px; /* Optional: add rounded corners */
                margin-left: 25px; /* Space between text and image */
            }
        </style>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
                            <header id="header">
                                <a href="self.html" class="logo"><strong>Yongkang Li </strong> homepage</a>
                            </header>
                        
                        	<!-- Banner -->
                            <section id="banner">
                                <div class="content">
                                    <header>
                                        <h1 style="font-size: 36px;">VisionNaviPro: Multimodal Visual Perception Solution for V2X</h1>
                                    </header>
                                </div>
                            </section>
                            <!-- <hr style="border-style: dashed; margin-top: -35px;" class="major" /> -->

							<!-- Section -->
							<section >						
								<div style="width: 100%; display: flex; justify-content: space-between; margin-top: -36px; margin-bottom: 24px;">
									<span class="image fit" style="flex: 1; margin-right: 10px;">
										<img src="../images/project/project1-VisionNaviPro/1.png" alt="" style="width: 100%; display: block; margin: 0 auto;"/>
									</span>
									<span class="image fit" style="flex: 1; margin-left: 10px;">
										<img src="../images/project/project1-VisionNaviPro/2.png" alt="" style="width: 100%; display: block; margin: 0 auto;"/>
									</span>
								</div>	
								<header class="main">
									<h1 id="introducation" style="font-size: 28px; margin-top: -20px; margin-bottom: 5px;">Introduction</h1>
								</header>
				
								<p style="font-size: 16px;">
									Perception is one of the most critical modules in autonomous vehicles, responsible for path planning within drivable areas to guide the vehicle safely. 
									However, as the vehicle moves, the conditions in drivable areas can change rapidly. Frequent path replanning alone cannot ensure high-speed driving stability, 
									making lane markings a key element in maintaining a steady course. Segmenting drivable areas, detecting lane lines, and identifying traffic objects are fundamental perception tasks. 
									However, executing these three vision-based tasks simultaneously demands significant computational resources, posing a challenge to the limited memory and processing power available in autonomous vehicles. 
									A multitask perception algorithm can address this issue by efficiently allocating computational resources without compromising detection accuracy. 
								</p><p style="font-size: 16px;">
									Given that autonomous vehicles operate in complex and dynamic environments, they are often affected by adverse weather, low light conditions at night, and occlusion of objects, 
									all of which pose serious threats to safe driving. To tackle these challenges, we propose VisionNaviPro, an advanced visual perception model for autonomous driving. 
									This project aims to deliver high-precision perception and localization services by integrating pure visual perception, large language model analysis, and semantic localization technologies. 
									Key techniques such as multitask learning, cross-modal attention mechanisms, and multi-scale deformable attention modules are employed to significantly enhance the system's efficiency and accuracy, 
									while maintaining high detection precision even under constrained computational resources. 
								</p><p style="font-size: 16px;">
									VisionNaviPro provides robust technical support for vehicle-road collaboration, vehicle-to-everything (V2X) networks, and autonomous driving perception systems. 
									Through the visual perception module, autonomous vehicles can accurately identify and locate various objects on the road, including lane lines, traffic signs, and other traffic participants. 
									The inclusion of the semantic localization module enables the system to better understand user commands and make precise decisions and localization accordingly.
								</p><p style="font-size: 16px;">
									The proposed method has demonstrated excellent experimental results on challenging autonomous driving datasets such as nuScenes, Talk2Car, Argoverse, and BDD100K, proving the effectiveness of the network architecture. 
									Specifically, the object detection module achieved a 12% increase in accuracy and a 10.2% reduction in parameters compared to state-of-the-art (SOTA) models. The semantic localization module reached a detection score of 72.7, 
									improving accuracy by 13.8% over SOTA models. Experiments on the BDD100K dataset showed a 7.9% and 3.6% improvement in average precision for lane detection and object detection, respectively, compared to existing SOTA models.
									</p>	
							</section>

							<!-- Section -->
							<section>
								<div>
									<header>
										<h1 id="detailed-information" style="font-size: 28px; margin-top: -20px; margin-bottom: 12px;">Detailed Information</h1>
									</header>
								</div>

								<div>
								<p style="font-size: 16px;">1. <b>Vision Perception Module:</b> The Vision Perception Module in VisionNaviPro is designed to handle the crucial tasks of visual recognition in autonomous driving, 
									including object detection, drivable area segmentation, and lane detection. The module is built on a CSP-Darknet architecture which serves as the backbone for feature extraction, 
									allowing the system to efficiently process input images. The extracted features are then fed into multiple decoders, each tailored for a specific visual task:</p>
								
									<div class="container">
										<div class="text-container">
											<ul>
												<li style="font-size: 16px;"><b>Object Detection Decoder:</b> Utilizes a multi-scale feature pyramid network (FPN) to detect and classify objects such as vehicles, pedestrians, and traffic signs. 
													The design includes RoI pooling and refined anchor box generation to enhance detection accuracy.</li>
												<li style="font-size: 16px;"><b>Drivable Area and Lane Detection Decoders:</b> These decoders are based on semantic segmentation principles, employing bilinear interpolation to upsample feature maps back to the original image resolution. 
													This allows for pixel-level classification, determining whether areas are drivable or belong to lane markings.</li>
											</ul>
										</div>
										<div class="image-container">
											<span class="image fit">
												<img src="../images/project/project1-VisionNaviPro/3.png" alt=""/>
											</span>
										</div>
									</div>
								</div>
								<div>
									<div class="container">
										<div class="text-container">
											<p style="font-size: 16px;">2. <b>Large Language Model Analysis Module:</b> This module leverages a fine-tuned large language model (based on LLama-7B) to provide deep semantic analysis and risk assessment in autonomous driving. 
												The integration of this module enables the system to understand complex textual descriptions, such as traffic signs, road conditions, and emergency alerts. The language model can interpret and analyze these descriptions in real-time, 
												offering a higher level of contextual awareness that enhances the vehicle's decision-making capabilities. This module is crucial for processing natural language commands and generating detailed risk assessments, 
												helping the vehicle anticipate and respond to potential hazards more effectively.</p>
										</div>
										<div class="image-container">
											<span class="image fit">
												<img src="../images/project/project1-VisionNaviPro/4.png" alt=""/>
											</span>
										</div>
									</div>
								</div>
								<div>
									<p style="font-size: 16px;">3. <b>Semantic Localization Module:</b> This module leverages a fine-tuned large language model (based on LLama-7B) to provide deep semantic analysis and risk assessment in autonomous driving. 
										The integration of this module enables the system to understand complex textual descriptions, such as traffic signs, road conditions, and emergency alerts. The language model can interpret and analyze these descriptions in real-time, 
										offering a higher level of contextual awareness that enhances the vehicle's decision-making capabilities. This module is crucial for processing natural language commands and generating detailed risk assessments, 
										helping the vehicle anticipate and respond to potential hazards more effectively.</p>
								</div>
							</section>

							<!-- Section -->
							<section>
								<div>
									<div>
										<header>
											<h1 id="project-achievements" style="font-size: 28px; margin-top: -20px; margin-bottom: 10px;">Project Achievements</h1>
										</header>
									</div>
									<div style="width: 100%;">
										<span class="image fit">
											<img src="../images/project/project1-VisionNaviPro/6.png" alt="" style="width: 90%; display: block; margin: 0 auto;"/>
										</span>
									</div>

									<p style="font-size: 16px;">The VisionNaviPro project has achieved significant advancements in the domain of autonomous driving, particularly through its innovative approaches in visual perception, large language model analysis, and semantic localization. 
										Here's a detailed summary of the project's achievements, focusing on experimental results, improvements, and the unique features of the system:</p>
									
									<div>
										<div class="container">
											<div class="text-container">
												<p style="font-size: 16px;">1. <b>Enhanced Accuracy and Efficiency in Visual Perception</b>VisionNaviPro employs a multitask learning framework combined with cross-modal attention mechanisms and multi-scale deformable attention modules. 
													This approach has dramatically improved the system's efficiency and accuracy, particularly in complex environments. Key achievements include:</p>			
												<ul>
													<li style="font-size: 16px;"><b>12% Increase in Detection Accuracy</b>: The object detection module outperformed state-of-the-art (SOTA) models by achieving a 12% increase in accuracy while also reducing parameter count by 10.2%.</li>
													<li style="font-size: 16px;"><b>Significant Gains in Lane and Object Detection</b>: Experiments on the BDD100K dataset showed a 7.9% improvement in lane detection accuracy and a 3.6% improvement in object detection accuracy compared to existing SOTA models.</li>
												</ul>
											</div>
											<div class="image-container">
												<span class="image fit">
													<img src="../images/project/project1-VisionNaviPro/5.png" alt=""/>
												</span>
											</div>
										</div>
									</div>

									<div>
										<p style="font-size: 16px;">2. <b>Superior Performance in Challenging Datasets: </b>The system was rigorously tested on several challenging public autonomous driving datasets, including nuScenes, Talk2Car, Argoverse, and BDD100K. 
											The results demonstrated the model's effectiveness across various scenarios:
										This module achieved a detection score of 72.7, marking a 13.8% improvement over SOTA models, showcasing its capability in accurately interpreting and localizing user commands in real-time.</p>
										<p style="font-size: 16px;">3. <b>Robust Support for Vehicle-Road Collaboration: </b>VisionNaviPro provides a comprehensive perception system that is highly suitable for vehicle-road collaboration and connected vehicle networks. 
											The system’s ability to accurately identify and locate various road objects—including lane markings, traffic signs, and other participants—enhances the safety and efficiency of autonomous vehicles. This is particularly critical for the development of intelligent transportation systems.</p>
										<p style="font-size: 16px;">4. <b>Practical Implementation and System Stability: </b>The project has been implemented on a robust technical platform, including Linux cloud servers and MySQL databases, ensuring stable operation and secure data storage. 
											This implementation underlines the system's readiness for real-world applications, where reliability and data security are paramount.</p>
									</div>
								</div>
							</section>
						</div>
					</div>

					<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Search -->
								<section id="search" class="alt">
									<form method="post" action="#">
										<input type="text" name="query" id="query" placeholder="Search" />
									</form>
								</section>

								<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
										<!-- <li><a href="index.html">Homepage</a></li> -->
                                        <li>
											<a href="../index.html" class="opener">Homepage</a>
											<ul>
												<li><a href="../index.html#self-introduction">SELF-INTRODUCTION</a></li>
												<li><a href="../index.html#news">NEWS</a></li>
												<li><a href="../index.html#research">RESEARCH</a></li>
                                                <li><a href="../index.html#software-copyright">SOFTWARE COPYRIGHT</a></li>
                                                <li><a href="../index.html#project">PROJECT</a></li>
												<li><a href="../index.html#scholarship-and-awards">SCHOLARSHIP AND AWARDS</a></li>
                                                <li><a href="../index.html#leadership-and-activities">LEADERSHIP AND ACTIVITIES</a></li>
											</ul>
                                        </li>
                                        <li>
                                            <a href="../publication/publication.html" class="opener">Publication</a>
											<ul>
												<li><a href="../publication/publication.html#2024">2024</a></li>
                                                <ul>
                                                    <li><a href="../publication/paper/aap.html">When, Where, and What? A Benchmark for Accident Anticipation and Localization with Large Language Models</a></li>
                                                    <li><a href="../publication/paper/ecai24.html">Less is More: Efficient Brain-Inspired Learning for Autonomous Driving Trajectory Prediction</a></li>
                                                    <li><a href="../publication/paper/aap.html">Real-time Accident Anticipation for Autonomous Driving Through Monocular Depth-Enhanced 3D Modeling</a></li>
                                                    <li><a href="../publication/paper/tiv.html">A Cognitive-Based Trajectory Prediction Approach for Autonomous Driving</a></li>
                                                </ul>
												<li><a href="../publication/publication.html#2023">2023</a></li>
                                                <ul>
                                                    <li><a href="#">Lightweight Human Observation-inspired Trajectory Prediction For Autonomous Driving</a></li>
                                                </ul>
											</ul>
										</li>
									</ul>
								</nav>

						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/browser.min.js"></script>
			<script src="../assets/js/breakpoints.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/main.js"></script>

	</body>
</html>